<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="torch多卡训练3-实战, Kaka">
    <meta name="description" content="本篇摘抄来自知乎大佬，仅仅用作学习
[TOC]
torch 多卡训练DDPSyncBN，支持多卡训练BN中有moving mean 和moving variance 这两个buffer更新依赖于当前batch得数据的计算结果，在普通多卡DP">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>torch多卡训练3-实战 | Kaka</title>
    <link rel="icon" type="image/jpeg" href="/medias/logo.jpg">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 6.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.jpg" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Kaka</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.jpg" class="logo-img circle responsive-img">
        
        <div class="logo-name">Kaka</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">torch多卡训练3-实战</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                          <div class="article-tag">
                            <span class="chip bg-color">无标签</span>
                          </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/DL-ML/" class="post-category">
                                DL/ML
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2022-09-08
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.min.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <p>本篇摘抄来自<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/250471767">知乎大佬</a>，仅仅用作学习</p>
<p>[TOC]</p>
<h3 id="torch-多卡训练DDP"><a href="#torch-多卡训练DDP" class="headerlink" title="torch 多卡训练DDP"></a>torch 多卡训练DDP</h3><h4 id="SyncBN，支持多卡训练"><a href="#SyncBN，支持多卡训练" class="headerlink" title="SyncBN，支持多卡训练"></a>SyncBN，支持多卡训练</h4><p>BN中有moving mean 和moving variance 这两个buffer更新依赖于当前batch得数据的计算结果，在普通多卡DP下，各模型只能拿到自己的那部分计算结果，所以在DP模式下的的普通BN被设计为只利用主卡上的计算结果来计算<code>moving mean</code> 和 <code>moving variance</code> ，之后再广播给其他卡，，这样实际上BN的batch size 就只是主卡上的batch size，这样会限制模型的性能</p>
<h5 id="syncBN原理"><a href="#syncBN原理" class="headerlink" title="syncBN原理"></a>syncBN原理</h5><p>分布式通讯接口在各卡间通讯，利用所有数据进行bN计算。 并且只传输各自进程的小 <code>batch mean</code> 和 <code>batch variance</code> </p>
<p>前向传播：</p>
<ul>
<li>在各自进程上计算各自的batch mean 和batch variance</li>
<li>对各自进程的mean 和variance进行 all_gather 操作，每个进程都得到s的全局量<ul>
<li>传递mean 和variance，减少通讯量</li>
</ul>
</li>
<li>计算总的mean 和总体variance </li>
<li>延续正常的BN计算，从前向传播计算数据中得到的batch_mean 和batch_variance 在各卡间保持一直，所以running mean 和running variance 就能保持一直</li>
</ul>
<p>后向传播，和正常的一样</p>
<h5 id="syncBN和DDP关系"><a href="#syncBN和DDP关系" class="headerlink" title="syncBN和DDP关系"></a>syncBN和DDP关系</h5><p>当前pytorch 中SyncBn只在DDP单进程单卡模式中支持，syncBN需要用到all_gather这个分布式接口，这个接口需要初始化DDP环境</p>
<blockquote>
<p>DDP 初始化阶段：</p>
<p>d. 创建管理器reducer 给每个parameter注册梯度平均的hook （在c++中实现，即reducer.h)</p>
<p>e 为可能的syncBn做准备</p>
</blockquote>
<p>需要注意</p>
<ul>
<li>为可能的syncBn做准备，实际上就是检测当前DDP单进程单卡模式，如果不是，则会停止</li>
<li>SyncBN需要在DDP环境初始化后初始化，但是要在DDP模型前就准备好</li>
<li>SyncBn依赖了all_gather，这个分布式接口当前不支持单进程多卡或者DP模式</li>
</ul>
<h5 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h5><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># DDP init</span>
dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span><span class="token string">'nccl'</span><span class="token punctuation">)</span>

<span class="token comment"># 按照原来的方式定义模型，这里的BN都使用普通BN就行了。</span>
model <span class="token operator">=</span> MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 引入SyncBN，这句代码，会将普通BN替换成SyncBN。</span>
model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>SyncBatchNorm<span class="token punctuation">.</span>convert_sync_batchnorm<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

<span class="token comment"># 构造DDP模型</span>
model <span class="token operator">=</span> DDP<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">,</span> output_device<span class="token operator">=</span>local_rank<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>convert_sync_batchnorm 原理</strong></p>
<p>torch.nn.SyncBatchNorm.convert_sync_batchnorm会搜索model里面的每一个module，如果发现这个module是，或者继承了torch.nn.modules.bachnorm._batchNorm 就把它替换为syncBn，如果自己的normalization是自己定义的类，且没有继承过_batchNorm 那么convert_sync_batchnorm是不支持的，需要自己实现一个新的syncBN。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">convert_sync_batchnorm</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> module<span class="token punctuation">,</span> process_group<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
       <span class="token triple-quoted-string string">r"""Helper function to convert all :attr:`BatchNorm*D` layers in the model to
       :class:`torch.nn.SyncBatchNorm` layers.
       """</span>
       module_output <span class="token operator">=</span> module
       <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>modules<span class="token punctuation">.</span>batchnorm<span class="token punctuation">.</span>_BatchNorm<span class="token punctuation">)</span><span class="token punctuation">:</span>
           module_output <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>SyncBatchNorm<span class="token punctuation">(</span>module<span class="token punctuation">.</span>num_features<span class="token punctuation">,</span>
                                                  module<span class="token punctuation">.</span>eps<span class="token punctuation">,</span> module<span class="token punctuation">.</span>momentum<span class="token punctuation">,</span>
                                                  module<span class="token punctuation">.</span>affine<span class="token punctuation">,</span>
                                                  module<span class="token punctuation">.</span>track_running_stats<span class="token punctuation">,</span>
                                                  process_group<span class="token punctuation">)</span>
           <span class="token keyword">if</span> module<span class="token punctuation">.</span>affine<span class="token punctuation">:</span>
               <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                   module_output<span class="token punctuation">.</span>weight <span class="token operator">=</span> module<span class="token punctuation">.</span>weight
                   module_output<span class="token punctuation">.</span>bias <span class="token operator">=</span> module<span class="token punctuation">.</span>bias
           module_output<span class="token punctuation">.</span>running_mean <span class="token operator">=</span> module<span class="token punctuation">.</span>running_mean
           module_output<span class="token punctuation">.</span>running_var <span class="token operator">=</span> module<span class="token punctuation">.</span>running_var
           module_output<span class="token punctuation">.</span>num_batches_tracked <span class="token operator">=</span> module<span class="token punctuation">.</span>num_batches_tracked
       <span class="token keyword">for</span> name<span class="token punctuation">,</span> child <span class="token keyword">in</span> module<span class="token punctuation">.</span>named_children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
           module_output<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span>name<span class="token punctuation">,</span> cls<span class="token punctuation">.</span>convert_sync_batchnorm<span class="token punctuation">(</span>child<span class="token punctuation">,</span> process_group<span class="token punctuation">)</span><span class="token punctuation">)</span>
       <span class="token keyword">del</span> module
       <span class="token keyword">return</span> module_output<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="DDP-下的梯度累加的进一步加速"><a href="#DDP-下的梯度累加的进一步加速" class="headerlink" title="DDP 下的梯度累加的进一步加速"></a>DDP 下的梯度累加的进一步加速</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> 每次梯度累加循环
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>K<span class="token punctuation">)</span><span class="token punctuation">:</span>
        prediction <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>prediction<span class="token punctuation">,</span> label<span class="token punctuation">)</span> <span class="token operator">/</span> K  <span class="token comment"># 除以K，模仿loss function中的batchSize方向上的梯度平均，如果本身就没有的话则不需要。</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 积累梯度，不应用梯度改变，</span>
        <span class="token comment"># yb注：这个时候其实会all_reduce 一次</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 应用梯度改变</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>一次梯度累加会有k个step，每次梯度累加循环会进行k次all_reduce ，但事实上，每次循环只有一次optimizer.step(), 即只应用一次，也就是说，在每一次梯度累加循环中，我们其实只要进行一次gradient all_reduce 即可满足要求，有k-1 次all_reduce 被浪费掉了。</p>
<h5 id="如何加速"><a href="#如何加速" class="headerlink" title="如何加速"></a>如何加速</h5><p>关键在于取消前k-1 次梯度同步。所以DDP提供了一个暂时取消同步的context函数， no_sync()  在这个context下，DDP不会同步</p>
<p>代码</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> DDP<span class="token punctuation">(</span>model<span class="token punctuation">)</span>

<span class="token keyword">for</span> 每次梯度累加循环
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 前accumulation_step-1个step，不进行梯度同步，累积梯度。</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>K<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">:</span>
        <span class="token keyword">with</span> model<span class="token punctuation">.</span>no_sync<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            prediction <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
            loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>prediction<span class="token punctuation">,</span> label<span class="token punctuation">)</span> <span class="token operator">/</span> K
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 积累梯度，不应用梯度改变</span>
    <span class="token comment"># 第K个step，进行梯度同步</span>
    prediction <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>prediction<span class="token punctuation">,</span> label<span class="token punctuation">)</span> <span class="token operator">/</span> K
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 积累梯度，不应用梯度改变</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>more 优雅的写法</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> contextlib <span class="token keyword">import</span> nullcontext
<span class="token comment"># 如果你的python版本小于3.7，请注释掉上面一行，使用下面这个：</span>
<span class="token comment"># from contextlib import suppress as nullcontext</span>

<span class="token keyword">if</span> local_rank <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>
    model <span class="token operator">=</span> DDP<span class="token punctuation">(</span>model<span class="token punctuation">)</span>

optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>data<span class="token punctuation">,</span> label<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 只在DDP模式下，轮数不是K整数倍的时候使用no_sync</span>
    my_context <span class="token operator">=</span> model<span class="token punctuation">.</span>no_sync <span class="token keyword">if</span> local_rank <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span> <span class="token keyword">and</span> i <span class="token operator">%</span> K <span class="token operator">!=</span> <span class="token number">0</span> <span class="token keyword">else</span> nullcontext
    <span class="token keyword">with</span> my_context<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        prediction <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>prediction<span class="token punctuation">,</span> label<span class="token punctuation">)</span> <span class="token operator">/</span> K
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 积累梯度，不应用梯度改变</span>
    <span class="token keyword">if</span> i <span class="token operator">%</span> K <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="多机多卡环境下的inference加速"><a href="#多机多卡环境下的inference加速" class="headerlink" title="多机多卡环境下的inference加速"></a>多机多卡环境下的inference加速</h4><p>问题：如何通过多卡推理拿到inference结果</p>
<p>思路：把数据split到各个进程中，并把结果合并到一起</p>
<h5 id="新的data-sampler。帮助我们吧数据不重复的分到各个进程上，"><a href="#新的data-sampler。帮助我们吧数据不重复的分到各个进程上，" class="headerlink" title="新的data sampler。帮助我们吧数据不重复的分到各个进程上，"></a>新的data sampler。帮助我们吧数据不重复的分到各个进程上，</h5><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SequentialDistributedSampler</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>sampler<span class="token punctuation">.</span>Sampler<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Distributed Sampler that subsamples indicies sequentially,
    making it easier to collate all results at the end.
    Even though we only use this sampler for eval and predict (no training),
    which means that the model params won't have to be synced (i.e. will not hang
    for synchronization even if varied number of forward passes), we still add extra
    samples to the sampler to make it evenly divisible (like in `DistributedSampler`)
    to make it easy to `gather` or `reduce` resulting tensors at the end of the loop.
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dataset<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> rank<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> num_replicas<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> num_replicas <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token keyword">not</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">raise</span> RuntimeError<span class="token punctuation">(</span><span class="token string">"Requires distributed package to be available"</span><span class="token punctuation">)</span>
            num_replicas <span class="token operator">=</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> rank <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token keyword">not</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">raise</span> RuntimeError<span class="token punctuation">(</span><span class="token string">"Requires distributed package to be available"</span><span class="token punctuation">)</span>
            rank <span class="token operator">=</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dataset <span class="token operator">=</span> dataset
        self<span class="token punctuation">.</span>num_replicas <span class="token operator">=</span> num_replicas
        self<span class="token punctuation">.</span>rank <span class="token operator">=</span> rank
        self<span class="token punctuation">.</span>batch_size <span class="token operator">=</span> batch_size
        self<span class="token punctuation">.</span>num_samples <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>math<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">1.0</span> <span class="token operator">/</span> self<span class="token punctuation">.</span>batch_size <span class="token operator">/</span> self<span class="token punctuation">.</span>num_replicas<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>batch_size
        self<span class="token punctuation">.</span>total_size <span class="token operator">=</span> self<span class="token punctuation">.</span>num_samples <span class="token operator">*</span> self<span class="token punctuation">.</span>num_replicas

    <span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        indices <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># add extra samples to make it evenly divisible</span>
        indices <span class="token operator">+=</span> <span class="token punctuation">[</span>indices<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>total_size <span class="token operator">-</span> <span class="token builtin">len</span><span class="token punctuation">(</span>indices<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># subsample</span>
        indices <span class="token operator">=</span> indices<span class="token punctuation">[</span>self<span class="token punctuation">.</span>rank <span class="token operator">*</span> self<span class="token punctuation">.</span>num_samples <span class="token punctuation">:</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>rank <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>num_samples<span class="token punctuation">]</span>
        <span class="token keyword">return</span> <span class="token builtin">iter</span><span class="token punctuation">(</span>indices<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>num_samples<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="如何把结果合并在一起，all-gather"><a href="#如何把结果合并在一起，all-gather" class="headerlink" title="如何把结果合并在一起，all_gather"></a>如何把结果合并在一起，all_gather</h5><p>如果网络输出在不同的进程中有着一样的大小，那么问题就很好解决</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 合并结果的函数</span>
<span class="token comment"># 1. all_gather，将各个进程中的同一份数据合并到一起。</span>
<span class="token comment">#   和all_reduce不同的是，all_reduce是平均，而这里是合并。</span>
<span class="token comment"># 2. 要注意的是，函数的最后会裁剪掉后面额外长度的部分，这是之前的SequentialDistributedSampler添加的。</span>
<span class="token comment"># 3. 这个函数要求，输入tensor在各个进程中的大小是一模一样的。</span>
<span class="token keyword">def</span> <span class="token function">distributed_concat</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> num_total_examples<span class="token punctuation">)</span><span class="token punctuation">:</span>
    output_tensors <span class="token operator">=</span> <span class="token punctuation">[</span>tensor<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>all_gather<span class="token punctuation">(</span>output_tensors<span class="token punctuation">,</span> tensor<span class="token punctuation">)</span>
    concat <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>output_tensors<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token comment"># truncate the dummy elements added by SequentialDistributedSampler</span>
    <span class="token keyword">return</span> concat<span class="token punctuation">[</span><span class="token punctuation">:</span>num_total_examples<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="完整的例子"><a href="#完整的例子" class="headerlink" title="完整的例子"></a>完整的例子</h5><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">## 构造测试集</span>
<span class="token comment"># 假定我们的数据集是这个</span>
transform <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
        torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">]</span><span class="token punctuation">)</span>
my_testset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> 
        download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>
<span class="token comment"># 使用我们的新sampler</span>
test_sampler <span class="token operator">=</span> SequentialDistributedSampler<span class="token punctuation">(</span>my_testset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span>
testloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>my_testset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> sampler<span class="token operator">=</span>test_sampler<span class="token punctuation">)</span>

<span class="token comment"># DDP和模型初始化，略。</span>
<span class="token comment"># ......</span>

<span class="token comment"># 正式训练和evaluation</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>total_epoch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 训练代码，略</span>
    <span class="token comment"># .......</span>
    <span class="token comment"># 开始测试</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 1. 得到本进程的prediction</span>
        predictions <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> data<span class="token punctuation">,</span> label <span class="token keyword">in</span> testloader<span class="token punctuation">:</span>
            data<span class="token punctuation">,</span> label <span class="token operator">=</span> data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>local_rank<span class="token punctuation">)</span><span class="token punctuation">,</span> label<span class="token punctuation">.</span>to<span class="token punctuation">(</span>local_rank<span class="token punctuation">)</span>
            predictions<span class="token punctuation">.</span>append<span class="token punctuation">(</span>model<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span>
            labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>label<span class="token punctuation">)</span>
        <span class="token comment"># 进行gather</span>
        predictions <span class="token operator">=</span> distributed_concat<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>concat<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
                                         <span class="token builtin">len</span><span class="token punctuation">(</span>test_sampler<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">)</span>
        labels <span class="token operator">=</span> distributed_concat<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>concat<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
                                    <span class="token builtin">len</span><span class="token punctuation">(</span>test_sampler<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 3. 现在我们已经拿到所有数据的predictioin结果，进行evaluate！</span>
        my_evaluate_func<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="我们可以单向的把predictions-labels-集中到rank-x3D-0的进程，只在其进行evaluation-并且输出，参考连接-send-recv"><a href="#我们可以单向的把predictions-labels-集中到rank-x3D-0的进程，只在其进行evaluation-并且输出，参考连接-send-recv" class="headerlink" title="我们可以单向的把predictions labels 集中到rank&#x3D;0的进程，只在其进行evaluation 并且输出，参考连接,send,recv"></a>我们可以单向的把predictions labels 集中到rank&#x3D;0的进程，只在其进行evaluation 并且输出，参考<a href="https://link.zhihu.com/?target=https://pytorch.org/docs/stable/distributed.html">连接</a>,send,recv</h5><h4 id="保证DDP的性能，确保数据的一致性"><a href="#保证DDP的性能，确保数据的一致性" class="headerlink" title="保证DDP的性能，确保数据的一致性"></a>保证DDP的性能，确保数据的一致性</h4><h5 id="性能期望"><a href="#性能期望" class="headerlink" title="性能期望"></a>性能期望</h5><p>进程为N的DDP训练和累加为N其他配置完全相同的单卡是一样的，</p>
<p>如果有问题：检查进阶部分的check list ，最多可能是数据上面出现了问题</p>
<p>DDP训练的时候，数据的一致性必须保证：各进程拿到的数据，要像是accumulation 为N其他配置完全相同的单卡训练中同个accumulation循环中不同iteration 拿到的数据，如果各个进程拿到的数据是一样的，分布相似，造成训练护具质量的下降，最终导致模型性能下降</p>
<h6 id="容易出错的点"><a href="#容易出错的点" class="headerlink" title="容易出错的点"></a>容易出错的点</h6><p>为保证实验的可复现性，一般在代码开头声明一个随机数种子</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> random
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torch

<span class="token keyword">def</span> <span class="token function">init_seeds</span><span class="token punctuation">(</span>seed<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> cuda_deterministic<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>
    <span class="token comment"># Speed-reproducibility tradeoff https://pytorch.org/docs/stable/notes/randomness.html</span>
    <span class="token keyword">if</span> cuda_deterministic<span class="token punctuation">:</span>  <span class="token comment"># slower, more reproducible</span>
        cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span>
        cudnn<span class="token punctuation">.</span>benchmark <span class="token operator">=</span> <span class="token boolean">False</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>  <span class="token comment"># faster, less reproducible</span>
        cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">False</span>
        cudnn<span class="token punctuation">.</span>benchmark <span class="token operator">=</span> <span class="token boolean">True</span>
        

<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 一般都直接用0作为固定的随机数种子。</span>
    init_seeds<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>但是会出现以下的问题：</p>
<ol>
<li>DDP的N个进程都使用了一个随机数种子</li>
<li>生成数据的时候，如果使用了一席随机过程的数据扩充方法，那么，各个进程生成的数据会带有一定的同态性<ol>
<li>例如yolov5 采用了msoaic数据增强，从数据中随机采样三张图像与当前拼接在一起，如果使用了相同的随机数种子，各卡生成的图像中，除了原本的销图，其他三张小图都一模一样</li>
</ol>
</li>
<li>同态的数据，降低了训练数据的质量，降低了训练效率，最终得到的模型性能可能更低</li>
</ol>
<p>我们需要分配给不同的进程，不同的固定的随机种子</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    rank <span class="token operator">=</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 问题完美解决！</span>
    init_seeds<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> rank<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="和DDP有关的技巧"><a href="#和DDP有关的技巧" class="headerlink" title="和DDP有关的技巧"></a>和DDP有关的技巧</h4><p>进程有快有慢，只有在gradient all-reduce 的时候，快的进程才会等慢的进程，也就是进行同步，有哪些地方需要进行同步呢，例如下载数据</p>
<ul>
<li>需要在唯一一个进程中开启</li>
<li>其他进程下载完成，再去加载数据</li>
</ul>
<p>在torch.distributed 提供了一个barrier()的接口，利用它我们可以同步各个各个DDP进程，但使用barrier函数的时候DDP进程会在函数的位置进行当代，等所有的进程都跑到了barrier函数的的位置他们才会再次向下执行。</p>
<h5 id="只在某个进程执行，无须同步"><a href="#只在某个进程执行，无须同步" class="headerlink" title="只在某个进程执行，无须同步"></a>只在某个进程执行，无须同步</h5><p>只需要一个判断，用不到barrier()</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> rank <span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">:</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<h5 id="简单的同步："><a href="#简单的同步：" class="headerlink" title="简单的同步："></a>简单的同步：</h5><pre class="line-numbers language-python" data-language="python"><code class="language-python">code_before<span class="token punctuation">(</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>barrier<span class="token punctuation">(</span><span class="token punctuation">)</span>
code_after<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<h5 id="在某个进程中执行A操作，其他进程等待期执行完成后再执行B操作"><a href="#在某个进程中执行A操作，其他进程等待期执行完成后再执行B操作" class="headerlink" title="在某个进程中执行A操作，其他进程等待期执行完成后再执行B操作"></a>在某个进程中执行A操作，其他进程等待期执行完成后再执行B操作</h5><pre class="line-numbers language-text" data-language="text"><code class="language-text">if rank == 0:
    do_A()
    torch.distributed.barrier()
else:
    torch.distributed.barrier()
    do_B()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="在某个进程中优先执行A操作，其他进程等待其执行完成后再执行A操作"><a href="#在某个进程中优先执行A操作，其他进程等待其执行完成后再执行A操作" class="headerlink" title="在某个进程中优先执行A操作，其他进程等待其执行完成后再执行A操作"></a>在某个进程中优先执行A操作，其他进程等待其执行完成后再执行A操作</h5><p>利用<code>contextlib.contextmanager</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> contextlib <span class="token keyword">import</span> contextmanager
<span class="token comment"># 一个上下文管理器，</span>

<span class="token decorator annotation punctuation">@contextmanager</span> <span class="token comment"># 当使用了这个装饰器后，</span>
<span class="token keyword">def</span> <span class="token function">torch_distributed_zero_first</span><span class="token punctuation">(</span>rank<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Decorator to make all processes in distributed training wait for each local_master to do something.
    """</span>
    <span class="token keyword">if</span> rank <span class="token keyword">not</span> <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>barrier<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 这里的用法其实就是协程的一种哦。</span>
    <span class="token keyword">yield</span>  <span class="token comment"># 这里yield的资源会绑定给as，如果有的话，同时当with的语句执行后则会继续执行，也就是说理论上说，0进程的会先执行，再阻塞，而其他进程会等到0进程执行之后再进行阻塞</span>
    <span class="token keyword">if</span> rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>barrier<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> torch_distributed_zero_first<span class="token punctuation">(</span>rank<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token keyword">not</span> check_if_dataset_exist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        download_dataset<span class="token punctuation">(</span><span class="token punctuation">)</span>
    load_dataset<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>属实优雅</strong></p>
<h4 id="避免DDP带来的冗余输出"><a href="#避免DDP带来的冗余输出" class="headerlink" title="避免DDP带来的冗余输出"></a>避免DDP带来的冗余输出</h4><p>问题：输出是n倍</p>
<p>解法：<strong>logging 模块+输出信息等级控制</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> logging

<span class="token comment"># 给主要进程（rank=0）设置低输出等级，给其他进程设置高输出等级。</span>
logging<span class="token punctuation">.</span>basicConfig<span class="token punctuation">(</span>level<span class="token operator">=</span>logging<span class="token punctuation">.</span>INFO <span class="token keyword">if</span> rank <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">else</span> logging<span class="token punctuation">.</span>WARN<span class="token punctuation">)</span>
<span class="token comment"># 普通log，只会打印一次。</span>
logging<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"This is an ordinary log."</span><span class="token punctuation">)</span>
<span class="token comment"># 危险的warning、error，无论在哪个进程，都会被打印出来，从而方便debug。</span>
logging<span class="token punctuation">.</span>error<span class="token punctuation">(</span><span class="token string">"This is a fatal log!"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>注：这个地方看了之后要去看看那天看到的语言分离的复现代码，可以学习的还有很多</strong></p>
<h3 id="关于DDP-半精度，以及多张卡同时并行更加详细的解释"><a href="#关于DDP-半精度，以及多张卡同时并行更加详细的解释" class="headerlink" title="关于DDP+半精度，以及多张卡同时并行更加详细的解释"></a><strong><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_23981335/article/details/118668834">关于DDP+半精度，以及多张卡同时并行更加详细的解释</a></strong></h3><h5 id="多卡eval"><a href="#多卡eval" class="headerlink" title="多卡eval"></a>多卡eval</h5><p>修复了上述infer或者eval的时候各进程通信的时候只能相同的tensor的bug</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">distributed_concat_by_all_reduce</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> each_process_num<span class="token punctuation">,</span> local_rank<span class="token punctuation">,</span> if_all_reduce<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> master_rank<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    output_tensors <span class="token operator">=</span> <span class="token punctuation">[</span>
        torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>each_process_num<span class="token punctuation">[</span>rank_<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">*</span><span class="token builtin">list</span><span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>
            device<span class="token punctuation">)</span> <span class="token keyword">if</span> local_rank <span class="token operator">!=</span> rank_ <span class="token keyword">else</span> tensor
        <span class="token keyword">for</span> rank_ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

    output_tensors <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>output_tensors<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># 为reduce 做准备</span>

    <span class="token keyword">if</span> if_all_reduce<span class="token punctuation">:</span> <span class="token comment"># 使用哪个method</span>
        torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>all_reduce<span class="token punctuation">(</span>output_tensors<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span><span class="token builtin">reduce</span><span class="token punctuation">(</span>output_tensors<span class="token punctuation">,</span> dst<span class="token operator">=</span>master_rank<span class="token punctuation">)</span>
    <span class="token comment"># truncate the dummy elements added by SequentialDistributedSampler</span>
    <span class="token keyword">return</span> output_tensors<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>CUDA_VISIBLE_DEVICES&#x3D;0,1 python -m torch.distributed.launch –nproc_per_node&#x3D;2 test_all_gather.py</p>
<p>&#x2F;home&#x2F;ybandyx&#x2F;disk&#x2F;YouboData&#x2F;Coding&#x2F;Pycharm&#x2F;Study&#x2F;skills&#x2F;torch_skill&#x2F;ddp</p>
<p>CUDA_VISIBLE_DEVICES&#x3D;0,1 python -m torch.distributed.launch –nproc_per_node&#x3D;2 main.py</p>
<p>&#x2F;home&#x2F;ybandyx&#x2F;disk&#x2F;YouboData&#x2F;Coding&#x2F;Pycharm&#x2F;Match&#x2F;law_event_detect&#x2F;baseline_rewrite</p>
</blockquote>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">KAKA</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://youblei.github.io/2022/09/08/dlandml/torch-duo-qia-xun-lian-3-shi-zhan/">https://youblei.github.io/2022/09/08/dlandml/torch-duo-qia-xun-lian-3-shi-zhan/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">KAKA</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            <span class="chip bg-color">无标签</span>
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2022/09/26/cs/matplotlib/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/4.jpg" class="responsive-img" alt="matplotlib">
                        
                        <span class="card-title">matplotlib</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2022-09-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9B%B8%E5%85%B3/" class="post-category">
                                    计算机相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2022/09/07/dlandml/torch-duo-qia-xun-lian-2-jin-jie/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/7.jpg" class="responsive-img" alt="torch多卡训练2-进阶">
                        
                        <span class="card-title">torch多卡训练2-进阶</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2022-09-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/DL-ML/" class="post-category">
                                    DL/ML
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- 是否加载使用自带的 prismjs. -->
  <script type="text/javascript" src="/libs/prism/prism.min.js"></script>


<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2023</span>
            
            <a href="/about" target="_blank">YouBLEI</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/YouBLEI/" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>















    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
